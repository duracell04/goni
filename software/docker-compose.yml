version: "3.9"

services:
  llm-local:
    image: vllm/vllm-openai:latest
    container_name: goni-llm-local
    # Uncomment for NVIDIA GPUs
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: ["gpu"]
    command:
      [
        "python",
        "-m",
        "vllm.entrypoints.openai.api_server",
        "--model",
        "mistralai/Mistral-7B-Instruct-v0.3",
        "--host",
        "0.0.0.0",
        "--port",
        "8000"
        # Deterministic profile (audit/self-loop):
        # "--enable-deterministic-inference",
        # "--max-num-seqs",
        # "1"
      ]
    environment:
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      # - HF_TOKEN=... # optional
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models

  vecdb:
    image: qdrant/qdrant:v1.6.1-unprivileged
    container_name: goni-vecdb
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage

  orchestrator:
    build:
      context: ./kernel
      dockerfile: Dockerfile
    container_name: goni-orchestrator
    image: ghcr.io/duracell04/goni-http:latest
    command: ["/app/goni-http"]
    environment:
      - LLM_LOCAL_URL=http://llm-local:8000/v1
      # Set deterministic mode for audit/self-loop runs (slower, single-seq):
      # - LLM_DETERMINISTIC=true
      # - LLM_SEED=1234
      - QDRANT_HTTP_URL=http://vecdb:6333
      - QDRANT_GRPC_URL=http://vecdb:6334
      - RUST_LOG=info
    depends_on:
      - llm-local
      - vecdb
    ports:
      - "7000:7000"
